# EN705651-Summer24-LLM-Project

## Authors: Alec Shamula, Oluwatobi Ajide, Akhil Gupta, Solomon Gruse   

The development of Transformer-based models like GPT has significantly advanced the field of natural language processing, particularly in the domain of text generation. Building on this foundation, we introduce a series of novel GPT-based models designed to enhance text generation by incorporating advanced techniques for uncertainty quantification, prediction flexibility, and overconfidence mitigation. 

We present Smooth-GPT, a model that utilizes label smoothing to temper overconfident predictions, promoting more generalized and balanced outputs. Bayes-GPT extends the traditional GPT architecture by integrating a Bayesian linear layer, enabling the model to capture and express uncertainty in its predictions. Sig-GPT replaces the conventional softmax activation with a sigmoid layer, allowing for multi-label predictions and greater flexibility in generating diverse outputs. Finally, Bayes-Sig-GPT combines the strengths of both Bayesian inference and sigmoid activation, offering a novel approach to balancing uncertainty and prediction diversity.

These innovations represent a significant step forward in the development of more robust and adaptable language models, capable of generating high-quality, contextually appropriate text across a variety of applications. This repository and attached paper detail the design and implementation of these models, demonstrating their potential to push the boundaries of current text generation capabilities.


## Prerequisites

* Please ensure you have at least Python 3.10 installed.   
* Install all libraries in `requirements.txt`

  ```
      conda create --name myenv
      conda activate myenv
      cd /path/to/your/project
      pip/conda install -r requirements.txt
  ```


## Navigating Repository


## Instructions


## Recommended Resources



