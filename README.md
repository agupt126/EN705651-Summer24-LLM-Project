# EN705651-Summer24-LLM-Project

Authors: Alec Shamula, Oluwatobi Ajide, Akhil Gupta, Solomon Gruse   
Summary: 
In contemporary state-of-the-art large language models (LLMs), training typically involves a SoftMax activation layer to predict the most probable token for sequence completion. We propose to assess the viability of employing a Sigmoid activation layer, which is commonly utilized in traditional multi-label classification tasks, to train an LLM instead. Additionally, we will explore the use of Bayesian layers. Bayesian layers learn a distribution over the weights, allowing for multiple samples from this distribution to obtain the next token prediction. The probability distributions in Bayesian Neural Networks (BNNs) enable them to learn from the data and understand the confidence in what they have learned. This makes BNNs more robust and flexible, particularly when there is a limited volume of data or when the data contains noise.



## Project MVP Update: 07/30/2024

### 1.0 General Updates

### 2.0 Training Data

#### 2.1 Dataset


#### 2.2 Augmentation

### 3.0 GPT-2 Training/Testing


#### 3.1 Baseline


#### 3.2 Sigmoid


#### 3.3 Bayesian

### 4.0 Next Steps
